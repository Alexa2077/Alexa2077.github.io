[{"title":"VLM调研报告","date":"2025-08-04T13:00:49.000Z","url":"/2025/08/04/VLM%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/","categories":[[" ",""]],"content":" 概述 参考文献：视觉语言模型综述 计算机视觉（Computer Vision）核心在于通过算法赋予计算机”看懂”图像和视频的能力。传统CV技术体系围绕对视觉内容的单点理解与分析展开：基础任务如图像分类，通过深度学习方法判断图片主体类别；目标检测实现物体的定位与识别，用矩形框标记位置；语义分割将每个像素归类到特定对象类别，形成精细化的区域划分。应用层技术则包括光学字符识别（OCR）将图片中文字检测并识别出来，以及结合人脸检测、特征提取与比对的人脸识别系统等。 基础CV模型能力环环相扣、相互协作，共同构建起从感知到理解的完整视觉智能体系。然而，传统的CV模型往往基于卷积神经网络 (CNN)构建，并且是在有限类别的数据集合上针对特定任务进行训练的。模型往往无法超越其训练的任务或类别集。如果测试用例发生根本变化或需要添加新的类别，算法工程师则须收集和标记大量图像并重新训练模型。这是一个昂贵且耗时的过程。此外，CV 模型往往没有任何自然语言理解的能力。 为解决传统CV模型的泛化瓶颈与语义鸿沟，自监督学习和多模态学习范式逐渐兴起。2020年Google Research提出的Vision Transformer（ViT）首次将Transformer架构引入视觉领域，通过全局注意力机制建模图像块序列的关联性，突破CNN的局部归纳偏置限制，首次验证了Transformer在视觉任务上的可行性。 随后2021年，OpenAI发布CLIP模型（Contrastive Language-Image Pre-training），其整体结构为经典的“双塔结构”：视觉编码器采用ViT，文本编码器采用Bert。基于4亿规模图文对，采用对比学习框架对齐视觉与语言模态，实现了图像与文本的跨模态统一对齐，在零样本分类任务中显著超越传统监督模型，验证了”提示词工程”在开放域视觉任务中的可行性。 2023年，Salesforce提出的BLIP-2进一步推动多模态预训练效率，通过冻结预训练视觉与语言模型参数、引入轻量级查询Transformer桥接两模态，显著降低计算成本的同时保持强大跨模态对齐能力，为资源受限场景提供高效解决方案。这些技术突破为视觉与语言的多模态深度融合奠定基础，最终催生出能跨越模态边界进行开放式回答的视觉语言模型。 大型语言模型的出现标志着人工智能领域转型的开始，它们在文本信息处理上的能力极大地推动了这一进程。尽管LLMs在文本处理上表现出色，但它们主要限于处理单一模态的数据，即文本。这限制了它们在理解和生成涉及图像和视频等多模态数据方面的能力。为了克服LLMs的局限性，研究人员开发了视觉-语言模型（VLMs）。这些模型结合了视觉和文本信息，展示了在理解和生成涉及图像和文本的内容方面的卓越能力。 视觉语言模型 (Vision Language Model, VLM) 是通过将大语言模型 (LLM) 与视觉编码器（Vision Encoder）相结合构建的多模态 AI 系统，通过将视觉映射到语言，让LLM能够理解和处理视频、图像和文本，使模型具有“看”的能力。与传统的CV模型不同，VLM 不受固定类别集或特定任务 (如分类或检测) 约束。通过在大量文本和图像视频-文本对的语料上进行训练，VLM 可以用自然语言处理许多典型的视觉任务（OCR、图像分类、目标检测、图像分割、人脸识别等）以及新的生成式 AI 任务，例如视频摘要和图像问答等。 总体来说，视觉语言模型VLM从2023年开始迅猛发展。VLM模型的快速进步，离不开如下4个关键因素： ViT (Vision Transformer)模型架构的成熟：谷歌首次尝试将 Transformer 直接应用于计算机视觉，使得视觉和语言模态之间的特征融合变得更加容易和有效，为后续 VLM 的发展提供了重要的视觉特征提取方法 。 OpenAI CLIP(Contrastive Language-Image Pretraining)模型的成功：基于对比学习将图像和文本映射到一个共同的特征空间中，实现图像和文本之间的相互检索和理解。CLIP 的成功展示了多模态预训练的强大潜力。 生成式自回归语言大模型（LLM）的兴起：基于Next Token Predict范式的GPT系列生成式自回归语言大模型在自然语言处理领域取得了巨大成功，展现出强大的语言生成和理解能力。 硬件算力的提升和数据scaling law的广泛验证：GPU集群性能的指数级增长使千亿参数的多模态模型训练成为可能，同时数据规模与模型性能的scaling law被广泛验证，极大地推动业界系统性地优化算力分配与数据配比，大幅降低训练成本并加速模型迭代效率。 四者共同构建了视觉-语言跨模态对齐的技术闭环，使VLM在语义理解、指令跟随等核心能力上实现突破。值得注意的是，VLM模型的兴起并不代表传统CV模型的没落，很多专业CV任务，例如人脸识别，VLM在此类垂直任务下的效果并不能令人满意，往往这类场景还是需要专用的模型才能得到很好的处理。 VLM的应用： :::color21，图像字幕生成：VLMs可以自动为图像生成描述性文字，这在社交媒体、内容管理和辅助视觉障碍人士方面非常有用。 2，视觉问答（VQA）：VLMs能够理解自然语言问题并根据图像内容提供答案，这项技术可以用于教育软件、虚拟助手和交互式客户服务系统。 3，图像检索：通过理解和索引图像内容及其相关文本，VLMs可以用于改进图像搜索技术，提供更准确和相关的搜索结果。 4，内容创作：VLMs可以根据给定的文本描述生成图像或视频，这对于艺术创作、游戏设计和电影制作等领域具有创新性的影响。 5，文档理解：帮助翻译多模态内容，例如带有图片说明的文档材料。 6，医疗影像分析：在医疗领域，VLMs可以辅助医生分析医学影像，并结合相关病例报告或临床笔记，支持诊断过程。 7，自动驾驶：VLMs可以整合视觉传感器数据和语言指令，帮助自动驾驶系统更好的决策。 8，增强现实和虚拟现实：在AR和VR应用中，VLMs可以提供对用户视觉和语言输入的更深层次理解，从而创造更丰富的交互体验。 9，社交媒体分析：VLMs可以分析社交媒体上的图像和文本内容，以识别趋势、情感或其他有用的信息。 ::: 0 多模态大模型MM-LLM **视觉语言模型（Vision language model）属于多模态大模型（multi-modality LLM，MM-LLM）的子集**：在MM-LLM框架下，模型的整体结构往往包含如下图所示的5个部分： MM-LLM架构： 模态编码器（Modality Encoder）：将多模态的数据（文本、语音、图像）编码成向量空间特征，该模块通常是单独进行预训练的，典型的方法有基于CNN的Resnet，基于Transformer的ViT等。 输入投影层（Input Projector）：将模态编码器的输出映射到LLM的输入特征空间的适配层，一般模型结构比较简单，不同的多模态模型一般是随机初始化该模块的参数做冷启训练。典型的网络层：MLP，Cross-Attention等。 LLM主干网络（LLM Backbone）：LLM是经过预训练的模型，一般还要串联多个模块继续做Post-Pretrain和微调，使得模型能识别多模态的特殊token和多模态的特征输入。 输出投影层（Output Projector）：将LLM生成的数据，映射成Modality Generator 可理解的特征空间，一般是简单的Transformer层或MLP层。 模态生成器（Modality Generator）：多模态的生成器，最终输出多模态的结果如图像、语音、视频等。模型基本都是基于LDM（Latent Diffusion Models）的衍生模型，如图片领域的Stable Diffusion方法。 模型整体以LLM为核心主干，分别在前后有一个输入、输出的投影模块（Projector），投影模块主要是用于桥接不同模态输入和输出。输入投影模块（Input Projector）用于将模态编码器处理的不同模态特征映射到文本特征空间，以便输入给LLM；输出投影模块（Output Projector）用于将文本特征空间结果映射到模态生成器的输入空间，以引导模态生成器生成多模态结果。 1 VLM： VLM基础架构： + 由图像和文本编码器生成嵌入 + 在图像和文本融合层中进行融合 + 将融合向量通过LLM生成最终的视觉感知生成文本 以LLaVA为例进行分析： Large Language and Vision Assistant，LLaVA，作为最早一批出现的视觉语言大模型之一，其结构分为Vision Encoder、Projection和LLM三部分，其模型结构与训练策略对于现在的多模态模型和视觉语言模型的发展产生了巨大的影响，主要包含如下图所示的3个部分。 由图中所示，常见的VLM整体架构包含一个 Vision Encoder 模块，负责提取图片特征；一个 Projection，负责将图片特征映射到 LLM 可接受的特征空间；以及一个 LLM 负责处理输入，并生成相应的语言模态的输出。在推理时，图片会依次经过 Vision Encoder 的编码以及 Projection 的特征转换，并与经过 tokenizer 和 embedding 层的 instruction 进行拼接，作为 LLM decoder layer 的输入，从而进行 LLM 的前向过程。某种意义上讲，Vision Encoder 和 Projection 所扮演的角色等价于视觉的 tokenizer + embedding 层。 LLaVA 的模型的训练分为两个阶段： 第一阶段，特征对齐阶段：只开放 Projection 的训练，冻结 Vision Encoder 与 LLM，从而让 Projection 模块学习到特征映射关系。此时训练数据通常为图像-文本对（如图像描述或简单问答），文本通过分词器转为 token 序列，与视觉特征经投影层对齐。 第二阶段，端到端微调过程：只冻结 Vision Encoder，开放 Projection 与 LLM 的训练，从而实现多模态的对话。此阶段采用多轮对话格式数据（如用户指令、模型回复、图像上下文），文本与视觉特征拼接后输入语言模型生成连贯响应。训练数据格式统一包含图像像素和文本序列，通过特殊符号（如）标记图像插入位置，实现模态融合。 VLM模型的训练数据格式统一包含图像像素和文本序列，通过特殊符号（如）标记图像插入位置，实现模态融合。 更具体地，LLaVA的Vision Encoder 方面选用了 CLIP-ViT-Large-Patch14（输入图片尺寸大小为 224px），LLM 方面选用了 Vicuna（基于 LLaMA 使用 ShareGPT 数据微调的模型），Projection 仅为一个MLP的线性层。随后相较于 LLaVA，LLaVA-1.5作为LLaVA的改进版，在数据和模型方面进行了进一步扩展，在性能上取得了进一步的提升。模型方面，LLaVA-1.5 将 Projection 由线性层更换为了一个两层的 MLP，将 Vision Encoder 更换为了 CLIP-ViT-Large-Patch14-336px。数据方面，LLaVA-1.5 加入了特定任务的数据集，以强化模型的表现。加入的数据集包括 VQA、OCR 以及区域感知数据。 （其实输入图片的尺寸大小是固定的？） 常见的VLM主要依赖如下3个核心技术： 视觉编码：通常采用大规模预训练的视觉表征模型（如CLIP、ViT），将输入图像转换为具有语义意义的视觉特征。这一模块的核心是将像素空间映射到与语言模态对齐的离散或连续表征空间，形成视觉token序列。 对齐机制：通过可学习的投影层（如线性层或轻量级适配器），将视觉特征空间与语言模型（如LLM）的嵌入空间对齐。典型方法包括两阶段训练策略：先在大规模图文对数据上学习粗粒度对齐，再通过指令微调实现细粒度的语义融合。 生成：基于预训练的语言模型（如GPT、LLaMA），通过指令微调（Instruction Tuning）或多模态思维链（Chain-of-Thought）技术，使模型具备视觉-语言联合推理能力，最终实现从视觉输入到自然语言输出的端到端生成。 VLMs的分类：根据VLM的输入处理和输出生成能力将其分为三个不同的组： 视觉语言理解模型：专门为视觉信息与语言的解释和理解而设计的模型 多模态输入文本生成模型：擅长利用多模态输入（如图像、视频和文本）来生成文本内容 多模态输入多模态输出模型：不仅接受多模态输入，还能产生多模态的输出 （千问的结构不是视觉语言理解模型，而是多模态输入文本生成模型。） 1.1 视觉语言理解 （Vision-Language Understanding, VLU）的VLMs专注于对**视觉信息与语言的解释和理解的结合**。 它们设计用来处理涉及图像和文本的复杂查询，例如视觉问答（VQA）和图像字幕生成。 VLU模型通常需要对图像内容有深入的理解，并且能够准确地用语言来描述或回答有关图像的问题。 CLIP 论文：《Learning Transferable Visual Models From Natural Language Supervision》 CLIP是一种神经网络，它通过自然语言指导来理解视觉概念。它能够识别多种基准上的视觉上的类别，展现出\"零样本\"（zero-shot）能力，即在没有看过特定类别样本的情况下也能识别它们。 通过对比学习的方式进行预训练，它将图像与其对应的文本描述进行对齐，从而学习视觉和语言之间的关联。 优势：对于分类任务，CLIP比一般的微调深度学习视觉模型具有更强的鲁棒性。 挑战：在抽象任务、细粒度分类、泛化和措辞敏感性方面仍存在困难。 CLIP是通过Contrastive Learning的方式来学习Vision和文本的表征。如图所示，对于一个Batch的数据，以样本集中原始图文pair$ &lt;I_i,T_i&gt; $为正例pair，Batch内与其他样本的$ I_x,T_x $组成为负例pair：$ &lt;I_i,T_x&gt;,&lt;I_x,T_i&gt; $ 。 模型训练采用了对比损失函数，通过最大化正例Pair的相似度，同时最小化负例Pair的相似度来训练模型。通过这种方式，能学习到视觉特征和文本特征的对齐关系。最后将训练好的Image Encoder模型(即ViT)参数保存下来，以供其他下游任务热启使用。 CLIP的训练使用了大量成对的图像和文本数据。这些数据对通常包括一个图像及其相关的描述或标题。图像编码器将输入的图像转换成一个固定大小的特征向量，而文本编码器将输入的文本描述转换成另一个固定大小的特征向量。CLIP的核心是对图像和文本特征向量进行对比学习。**模型试图将与图像内容相匹配的文本描述的特征向量拉近，同时将不匹配的文本描述的特征向量推远。 ** GLIP 论文：《Grounded Language-Image Pre-trainin》 通过短语定位（phrase grounding）实现对象级别的对齐，这意味着模型能够将文本中的短语与图像中的具体对象联系起来。 GLIP 将对象检测重新定义为一种视觉-语言任务，通过深度融合（deep fusion）改进表示学习，从而提高模型对图像中对象的识别能力。 GLIP 能够利用语义丰富的数据进行可扩展的预训练，这使得模型能够自动生成定位框（grounding box），并且在零样本&#x2F;少样本（zero&#x2F;few-shot）迁移性方面表现出色。 GLIP 在多种视觉-语言任务上表现出色，包括图像字幕生成和下游目标检测任务。它能够与完全监督的动态头部（dynamic head）竞争。 GLIP的一个关键特性是短语定位，它通过将文本中的短语与图像中的对象匹配来实现。这有助于模型理解文本描述与视觉内容之间的对应关系。通过对比性学习框架进行预训练，其中包括正样本对和负样本对。模型被训练为将正样本对的嵌入拉近，同时将负样本对的嵌入推远。通过定位损失，实现对图像的所有目标的定位。 VLMO 论文：《VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts》 论文旨在解决现有视觉-语言模型在处理复杂分类任务时存在的局限性，以及在检索任务中的效率问题。现有模型通常采用双重编码器或融合编码器架构，但这些架构各有优缺点，无法同时满足高效检索和复杂分类任务的需求。 1.2 多模态输入的文本生成 Flamingo 论文：《Flamingo: a Visual Language Model for Few-Shot Learning》 传统的视觉模型通常需要大量的标注数据来进行特定任务的训练和微调。然而，获取大量标注数据成本高昂且耗时。Flamingo 旨在通过少样本学习（few-shot learning）来克服这一限制，即使在只有少量示例的情况下也能快速适应新的视觉和语言任务。 :::color21，多模态输入：Flamingo模型能够接收交错的视觉数据（如图像或视频）和文本作为输入。这允许模型在处理视觉信息的同时，也能够理解和生成语言。 2，少样本学习：图1展示了Flamingo模型如何通过少量的示例（即“shots”）快速适应各种图像和视频理解任务。例如，模型可能只看到一个或几个示例，然后就能够对新图像生成描述或回答问题。 3，开放式任务：Flamingo能够处理开放式任务，如视觉问答（Visual Question Answering, VQA），在这些任务中，模型被提示一个问题，并需要生成答案。例如，如果输入是一个图像和一个问题，模型将生成一个答案。 4，描述任务：模型还能够描述场景或事件，如图1中的“这是一个有粉红色火烈鸟的房间，有火烈鸟形状的泳池浮床”。 5，视觉对话：Flamingo能够参与多图像的视觉对话，如图1底部所示，模型能够根据提供的图像和对话历史生成合适的回复。 6，数学问题解答：图1还展示了模型能够解决简单的数学问题，如2+1&#x3D;3，5+6&#x3D;11等。 7，文本生成：Flamingo能够根据给定的文本提示生成新的文本，例如，根据提示“我喜欢阅读，我最喜欢的剧本是《哈姆雷特》。我也喜欢…”，模型生成了“…我最喜欢的书是《我父亲的梦想》”。 8，图像识别和描述：模型能够识别图像中的对象，并根据这些对象生成描述，如图1中的“这是一只栗鼠。它们主要在智利发现。” ::: 模型架构： 从整体架构来看，火鸟有几个组件： Visual Encoder 模块； Perceiver Resampler 模块； 通过门控注意力机制 GATED XATTN-DENSE 实现的 Adapter 模块； 语言模型 ChinChilla； **模型整体运行流程（将视觉皈依到语言中）**：首先通过Visual Encoder 来采集视觉信息，这里视觉信息包括 图像和视频，然后使用 Perceiver Resampler 将视觉信息进行汇总，形成统一输出。紧接着，通过 Adapter模块（GATED XATTN-DENSE）将视觉信息 “嵌入” 到语言模型中，这样，就可以利用语言模型的优势来做多模态任务了。 Visual Encoder模块（类似clip的编码器）：使用的是 NormalizerFree ResNet (NFNet)，这个模块使用和 CLIP 一样的训练方法进行训练。之后，在火烈鸟模型中，该模块的权重会被冻结，只起到一个视觉特征提取功能，注意，上文中提到，将 CLIP 的 Visual Encoder 用在其他任务中作为特征提取模块，效果非常好，相信同样采用 CLIP 训练方法训练的 NFNet 也能具有同样的效果。 Perceiver Resampler模块：这个模块的作用是将图片以及不同尺寸的视频，进行统一的表征建模，从而确保其具有统一维度的输出，这样最大的好处就是模型既可以支持图片和视频，也可以保证在训练的时候，可以进行批量训练。 在视觉表示上调节冻结的语言模型，将视觉信息注入预训练的语言模型。 语言部分使用的是DeepMind之前提出的具有70B参数的自回归语言模型ChinChilla。该模型和GPT系列一样，使用的是Transformer 的Decoder作为基础框架来训练的。这个模型在训练过程中是冻结的，即不更新其权重。这样做是为了避免在训练新任务时发生灾难性遗忘（catastrophic forgetting），即模型忘记了之前学到的知识。 在冻结的语言模型层之间，插入了新的交叉注意力层，这些层是从头开始训练的。这些层被称为 GATED XATTN-DENSE 层，它们能够将视觉信息整合到文本生成过程中。 这些层由两部分组成：交叉注意力（Cross-Attention）和密集前馈网络（Dense Feed-Forward Network）。交叉注意力层使用来自感知重采样模块的视觉特征作为键（keys）和值（values），而语言输入作为查询（queries）。 GATED XATTN-DENSE 层使用了一个特殊的门控机制，通过 tanh 激活函数对新层的输出进行缩放。这个门控参数在初始化时设置为0，这意味着在训练开始时，新层的输出不会影响模型的输出，保持了冻结语言模型的完整性。 交叉注意力层使得模型能够在生成文本时考虑视觉输入，例如图像或视频内容。这允许模型生成与视觉场景相关的描述或回答视觉相关问题。 在上文中提到，这样做的好处是，在不破坏预训练语言模型内部“知识”的情况下，可以无缝的将视觉信息“嵌入”进来，从而可以有效的利用语言模型强大的推理能力，帮助实现多模态推理。 多视觉输入支持：（不太理解）每张图像&#x2F;视频注意力掩蔽 为了有效处理多个视觉输入，Flamingo 使用了一种注意力掩蔽技术。这种技术通过限制模型在每个文本标记上可以看到的视觉标记的数量，来控制模型的注意力。 在给定的文本标记上，模型只关注在该文本标记之前出现的最后一张图像的视觉标记。这种设计允许模型专注于与当前文本标记最相关的视觉信息。 尽管模型在任何给定的文本标记上只直接关注一张图像，但通过语言模型中的自注意力机制，模型仍然可以隐式地依赖之前的所有图像。 :::color2火烈鸟总结： 1，将视觉 “嵌入” 到语言模型中，这样就可以利用语言模型的知识来进行推理。 2，通过 Pereciver Resampler模块，为图片和不同时长的视频提供了一个统一的表征维度。 3，使用Adapter，在训练时，只调整Adapter，保证在不破坏语言模型所存储的知识的同时，利用语言模型进行推理，同时，由于无需调整语言模型参数，该方法非常节省计算资源。 4，通过交叉输入图片（视频）和文本的方式，训练模型，使其具有 few-shot 的多模态序列推理能力。 ::: BLIP：（有些不理解） 论文：《BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation》 现有的视觉-语言预训练模型通常只在理解型任务或生成型任务上表现优异，而BLIP旨在两者上都有所突破。现有方法通过扩大从网络收集的含噪声的图文对数据集来提升性能，但这种方法得到的监督信号并不理想。 BLIP框架通过多模态混合编码器-解码器（MED）模型和数据自举方法（CapFilt），在预训练阶段生成高质量合成标题并滤除噪声，实现在多种视觉-语言任务上的统一理解和生成能力。 多模态混合编码器-解码器： （1）单模态编码器：分别编码图像和文本。文本编码器与 BERT相同，其中 [CLS] 标记附加到文本输入的开头以总结句子。 （2）基于图像的文本编码器，通过在文本编码器的每个变压器块的自我注意(SA)层和前馈网络(FFN)之间插入一个额外的交叉注意(CA)层来注入视觉信息。将特定于任务的 [Encode] 标记附加到文本中，[Encode] 的输出嵌入用作图像-文本对的多模态表示。 （3）基于图像的文本解码器，它将基于图像的文本编码器中的双向自注意力层替换为因果自注意力层。[Decode] 令牌用于对序列的开头进行信号，并使用序列结束标记对其结尾进行信号。 BLIPv2： 论文：《BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models》 解决的问题：BLIP-2旨在解决视觉-语言模型在预训练期间的高计算成本问题，尤其是在使用大规模模型和数据集进行端到端训练时。此外，它还解决了在冻结大型语言模型（LLMs）的情况下，如何有效地进行视觉-语言对齐的挑战。 创新点： 两阶段预训练策略：BLIP-2提出了一个新颖的两阶段预训练方法，首先从冻结的图像编码器中学习视觉-语言表示，然后利用冻结的语言模型进行视觉到语言的生成学习。 轻量级查询变换器（Q-Former）：Q-Former是一个轻量级的变换器，使用一组可学习的查询向量从冻结的图像编码器中提取视觉特征，作为冻结图像编码器和冻结LLM之间的信息瓶颈。 总结 参考： 从Flamingo看多模态研究发展趋势: &#x2F;&#x2F;mp.weixin.qq.com&#x2F;s&#x2F;okLlk713ZR3QHtuq3-Nvag 当前大模型比较流行的两大趋势： 1，从预训练—&gt;微调（模型适应下游任务）的思维，逐渐转变到 预训练—&gt; Prompt&amp;Adapter（下游任务适应模型）的思维，而这也是当前领域正在发生的第四范式。很自然，这一范式也被逐渐引入到了多模态领域，这也是当前多模态领域发展的一个大趋势。 2，预训练语言模型本身就是一个巨型知识库，因此，在各类视觉语言任务中，通过将视觉信息“嵌入”到语言模型中，利用语言模型里面的知识来做推理则是另外一个大趋势。 参考 《VLM综述》 2 开源视觉语言模型梳理 仔细阅读和理解：[]() 主流的VLM基座大模型：Qwen2.5-VL、QVQ-72B-Preview、Llama3.2-Vision、InternVL2.5-MPO主要是在服务端运行的超大VLM模型。 中小尺寸的轻量化多模态模型：MiniCPM-o、Phi-4-multimodal、Gemma3主要是中小尺寸的多模态模型，其模型设计初衷只希望在算力有限的设备上运行，同时部分模型还支持音频输入与生成。 Qwen视觉语言模型梳理 参考：[]() 【把Qwen系列搞懂】 Qwen系列模型是由阿里巴巴开源的基座系列模型。是目前全球受众最广泛，影响力最大的基座模型之一。Qwen关于VLM模型发布分别为：Qwen-VL，Qwen2-VL，Qwen2.5-VL，共有8个不同尺寸的模型。 QWenVL出来的时间相对较晚，所以最开始就采用了以LLM为核心、将图像特征简单转换为文本特征的框架。其中，QWenVL是第一个版本，训练的模型较小（～9.6B），并且不支持高清分辨率的图像；QWen2-VL将模型的参数量增加到～70B，支持动态的分辨率；QWen2.5-VL将模型的数据量增加到4.1T token，并且借鉴了LLM中的RL和COT技术。 下面是几个版本的对比： 模型 QWen-VL QWen2-VL QWen2.5-VL 发布时间 2023.8 2024.9 2025.2 模型结构 Qwen-7B+ViT-bigG-1.9B+Adapter Vision Encoder：675M LLM：Qwen2:1.5B~72B Vision-Language Merger Vision Encoder: relatively fewer parameters LLM：Qwen2.5:3B~72B Vision-Language Merger Vision Encoder网络结构 OpenClip’s ViT-bigG DFN*vit + 2D RoPE Dynamic-resolution ViT+Window Attention*+2D RoPE Adaptor网络结构 Cross attention module MLP MLP 训练数据 1.4B+76.8M+ 350k 1.4 trillion tokens 4.1 trillion tokens 创新点 与主流的框架差别很小，工程上的贡献多于学术贡献 支持不同分辨率的图像输入 MRoPE VLM RLHF SFT+DPO MCOT 版本参数 Qwen-VL：9.6B Qwen2-VL-2B：2.2BQwen2-VL-7B：8.3BQwen2-VL-72B：73B Qwen2.5-VL-3B：3BQwen2.5-VL-7B：8.3BQwen2.5-VL-72B：73B 链接    注： 上表只列出了几个主要的版本，并没有列出一些衍生版本的模型，Qwen发布的模型还包括：量化、指令微调等模型版本 下面对QwenVL系列模型进行具体介绍和学习： Qwen-VL 论文：《Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond》 论文链接： Qwen-VL 于 2023 年 8 月发布，和LLaVA在模型结构设计和训练策略上有诸多不同：在大语言模型（LLM）方面，使用了来自 Qwen-7B 模型的预训练权重；视觉编码器采用 Vision Transformer（ViT）架构处理输入图像生成图像特征，训练和推理时会将输入图像调整到特定分辨率并分割成大小为 14 的图块，整个ViT的预训练权重源自 OpenAI 的 CLIP 的 ViT-bigG；Projector采用位置感知的视觉-语言Adaptor，用于缩短图像 token 长度并作模态映射，由单层交叉注意力模块构成，以一组可训练向量（嵌入）为查询向量、视觉编码器的图像特征为交叉注意力操作的键且随机初始化。 套用MM-LLM的框架，Qwen-VL包括3个典型的模块： 模态编码器（Modality Encoder） ： 视觉编码器（visual encoder），只用来编码图片视觉特征 输入投影层（Input Projector）：位置感知的适配器（position-aware adapter） LLM主干网络（LLM Backbone）： Qwen-7B Base 模型 下面我们分别来看看Qwen-VL的两个核心模块：**视觉编码器、感知位置的适配器。**接着描述一些规范化样本处理过程，最后描述下模型的训练过程**。** 视觉编码器 Visual Encoder Qwen-VL的视觉编码器使用的是ViT架构（Vision Transformer），ViT的网络设置和初始化参数使用了OpenCLIP预训练好的ViT-bigG模型。OpenCLIP是laion.ai组织的一个开源项目，是对OpenAI's的CLIP（Contrastive Language-image Pre-training）的开源实现。laion.ai发布了一系列基于CLIP框架训练的不同size模型。 Clip：huggingface模型： 在Qwen-VL中采用的是标准的ViT框架，ViT的原理比较简单：将图片分割成多个图像块（Patch），然后针对每个Patch通过一系列线性映射，转化成token，再将所有token拼接成序列，最终将一张图片从$ (H,W,C) $格式转换成$ (S,H) $格式的序列特征。其中 H：高，W：宽，C：通道数，S： 序列长度，H：特征维度。在标准的ViT实现上，输入图片会先被调成1:1长宽比的正方形，然后再分割成固定的图像块。 因此这种标准的ViT框架的设计，只能接收固定分辨率的图片，同时Patch的大小也是模型在训练期间使用的一个固定size。ViT处理过程如图所示： 根据上面的知识我们可以再看Qwen-VL的编码器过程（源码：）： 首先看下源码设置的一些参数： Qwen-VL 可接受的图像分辨率为448448，所以输入的图片会先处理成统一的尺寸：（W: 448, H: 448，C: 3）。注：Qwen-VL模型训练其实是做了三个阶段。第一阶段图像会统一处理成低像素224224，后面两个阶段统一分辨率为448*448。这里只以高分辨率的设置为例。 patch_size : 14 ，这个参数指定patch的大小，同时也是卷积核和的尺寸，也是卷积操作的stripe步长 width：1664，这个参数指定的输出通道数，即out_channels，也就是每个Patch输出的特征的维度我们以batch_size(B) &#x3D; 1为例 ViT核心处理就几行代码，如下： 代码注释1的处理过程：一张图片做卷积操作，处理成 [width, grid, grid] &#x3D; [1664, 32, 32]的数据，如图所示： 代码注释2的处理过程：按行优先展开，处理成一个二维格式的数据[sequence_len, hidden_size] &#x3D; [1024, 1664]（类似与一条文本处理后的序列）。如图下所示。 输入投影层：位置感知的适配器 Position-aware Vision-Language Adapter 经过上述ViT处理后，对于448*448分辨率的图像，生成一个[1024, 1664]的序列，也就是向量维度为1664的长度为1024的序列。为了压缩视觉token的输入长度，Qwen-VL引入了一个Adapter来压缩图像特征。这个Adaper就是一个随机初始化的单层**Cross-Attention模块**。该模块使用一组可学习的query向量，将来自ViT的图像特征作为Key向量。通过Cross-Attention操作后将视觉特征序列压缩到**固定的256长度**。 对于Transformer我们平时接触更多的是Self-Attention，在Self-Attention计算中q,k,v都是基于输入特征做矩阵变换后得到的，通常q,k,v的长度处理前后也是一样的。那么这里提到的Cross-Attention、可学习的query向量、做序列压缩等，针对这些描述是否真正理解了呢？ 如下图所示描述了基于可学习Query和ViT输出的序列作为k，v的Attention计算过程，经过Cross-Attention后，将ViT阶段的1024长度的序列，压缩到了长度为256的序列。 此外，考虑到位置信息对于精细图像理解的重要性，Qwen-VL将二维绝对位置编码（三角位置编码）整合到Cross-Attention的q,k中，以减少压缩过程中可能丢失的位置细节。随后将长度为256的压缩图像特征序列输入到大型语言模型中。 输入和输出 对于输入LLM前的特征序列，为了区分图片和文本的输入信息，对图片的feature使用了特殊的token包裹，图像特征的开始和结束用和token 圈定，来明确标识图像特征的起止位置。同时为了做grounding任务，对图像中bounding box 统一用一个\"左上-右下\"坐标框格式表示：\" $ (X_topleft,Y_topleft),(X_bottomright,Y_bottomright) $。坐标值统一做归一化处理，规范化到（0，1000）区间。并用,两个特殊的token圈定起来。下面图7是一条典型的grounding 任务的样本实例： 训练过程 Qwen-VL 的训练流程包含3个阶段，分别为预训练、多任务预训练和监督微调。 第一阶段，单任务大规模预训练：冻结大语言模型，仅优化视觉编码器和基于Cross Attention构建的 VL Adaptor 。利用大规模、弱标记、网络爬取的图像文本对（清洗后 14 亿对，77.3% 为英语、22.7% 为中文），训练数据的图片统一处理成224*224的尺寸。 第二阶段，多任务预训练：解锁大语言模型，训练整个模型。引入高质量、细粒度的 VL 注释数据，在 7 个任务（如字幕、视觉问答、定位等）上同时训练，使用多个公开数据集和内部语料库，该阶段的训练数据，Vision数据的分辨率从224提升到448。 第三阶段，SFT有监督微调：冻结视觉编码器，优化语言模型和适配器模块。多模态指令调整数据来自字幕或对话数据，还构建额外对话数据，混合多模态和纯文本对话数据（35 万条）。通过指令微调增强指令跟随和对话能力，得到 Qwen-VL-Chat 模型。 Qwen2-VL 链接： 相对于Qwen-VL，Qwen2-VL整体模型架构做的比较大的升级，首先从模型命名上可知，主体模型从Qwen升级到了Qwen2。并且发布了三个size的模型，分别是Qwen2-VL-2B，Qwen2-VL-7B，Qwen2-VL-72B。 注： 1. Qwen2-VL系列模型，针对Vision Encoder采用了相同size的模型结构，这里应该是做了一些ablation的实验，取得一个合适的size。2. 另外相对于Qwen-VL系列，Qwen2-VL并没有显示描述Vision-Language Adapter的参数，通过查看源码，Qwen2-VL对Adapter做了简化处理，并没有采用一个Cross-Attention的结构，而是使用了简单的线性变换层，这层参数比较少，相对于总参数规模，可以忽略不计。 主要有以下重要的升级点： 1，**采用原生动态分辨率：单一分辨率 -&gt; 任意分辨率，** Qwen-VL模型输入只接受单一分辨率的图片，Qwen2-VL 引入了朴素动态分辨率机制，能处理任意分辨率的图像，将其动态转换为可变数量的视觉令牌。通过修改了 ViT结构，去除原始绝对位置嵌入，引入 2D-RoPE 来捕获图像二维位置信息。在推理阶段，不同分辨率图像被打包成单个序列，还通过 MLP 层压缩视觉token，提高了处理效率和对不同分辨率图像的适应性。 2，Vision Encoder位置编码：绝对位置编码 -&gt; 相对位置编码，从二维三角位置编码升级到二维RoPE位置编码，RoPE对长序列有更好的泛化能力，有利于提升对长序列Vision特征的建模能力。 3，LLM主体模型位置编码：1D-&gt;3D RoPE，引入多模态旋转位置编码技术（M-RoPE），刻画多模态(时序、高、宽)三维数据。进一步提升对时空数据的建模能力。 4，统一多模态数据： 单图片 -&gt; 统一图片和视频，统一框架处理图片和视频数据，进一步提升对真实世界认知和理解能力。 5，训练数据： 1.4B -&gt; 1.4T，数据量提升了3个量级，同时数据覆盖了多领域任务。 下面详细介绍下这些升级点： 原生动态分辨率 Naive Dynamic Resolution Qwen-VL使用的视觉编码器是标准的ViT，这要求输入的图片要统一处理成单一的、固定的分辨率，才能feed到模型进行处理。一般标准的预训练好的ViT，通常是将图片处理成正方形（长:宽=1:1）。这样处理后通常图片会失真，导致模型理解上有信息损失或引入一些误导。如下图所示： 左侧是传统的ViT对输入的处理（也是Qwen-VL采用的方法），对于一些宽高比差距较大的图片，处理后通常会造成图片扭曲，而Qwen2-VL实现的原生动态分辨率方法则会保留原始图片的宽高比，将图片resize到适当的大小，图片像素满足[min_pixel, max_pixel]区间,再对图片做Patch处理，将每个图片处理成变长的Vision token序列，再输入给LLM模型。 目前看上述的方法是比标准的ViT更合理的，因为它保留了图片的原始分辨率，但是同时也引入了一个问题。 问题是这样：传统的ViT会将任何图片数据都处理成定长的Patch序列，然后输入给Vision Encoder，这种统一定长的输入是对硬件计算非常友好的，非常好组Batch，并且不需要任何padding处理。Batch序列中每个位置的计算都是有效的。而对于上面提到的原生动态分辨率方法会将不同图片处理成不同长度的Patch序列。对于不同的长度的输入，做并行计算时，我们自然会想到类似于文本数据的操作，对数据做padding，再Feed给模型。但这相比传统的ViT方法（无Padding）会更慢（因为为了适配一个Batch中最长的序列，要做适当的Padding处理，导致会有些冗余计算）。因此这并不是一个完美的方法。Qwen2-VL采用的原生动态分辨率方法实现上同时也考虑了性能问题。 那么原生动态分辨率方法具体是怎么实现的呢？ 核心方法是采用了NaViT的Patch Pack技术，把不同图像的多个patch打包到一个序列，能保留不同图片的可变分辨率。同时在一个次序列计算中同时可处理多个图像，提升了模型计算的吞吐，在性能上始终优于传统的ViT。其性能提升主要来源于Pack处理后，一个序列包括多个图片能同时计算，使得在固定计算预算下，动态分辨率方法能训练更多样本，从而带来更好的性能。 那么一个序列中塞进了多个图像数据，怎么能互不干扰的计算呢（也就是在做ViT的Attention计算时，多个图片的Patch在一个序列中需要做计算隔离） 我们以一个简单例子描述下动态分辨率方法的处理逻辑。 举例：假设我们5张图片:$ I_1-I_5 $，且patch长度为2~6，即图片Patch后长度为： $ {I_1:2,I_2:3,I_3:4,I_4:5,I_5:6} $ 。为了描述简单，我们假设模型设置Batch_Size&#x3D;2，并且正好处理这5张图片到一个Batch中。 处理过程： a）首先我们将5张图片进行Pack，放到2个序列中 一个很简单的方式是将3个Patch较短的图片放到一个序列$ S_1 $，2个较长Patch的图片放到一个序列$ S_2 $。符号化为：Batch &#x3D; $ ({S_1,S_2}) $ ，其中 $ S_1&#x3D;(I_1:2,I_2:3,I_3:4) $ 序列长度为 9，$ S_2 &#x3D; (I_4:5,I_5:6) $序列长度为11。 b) Batch内做序列Padding对齐处理 根据Batch内最长序列，通过F.pad方法做序列对齐，在序列前后增加Padding token，该例子中由于 较短，需要在末尾增加Padding token，处理后，如下图所示 c) 通过设置Attention Mask保证同Sequence中各图片计算隔离 一个序列中有多张图片输入，在计算时要必须保证各图片的Attention计算是相互隔离的。实现上通过对Attention Mask矩阵做特殊的设置，来保证计算隔离。计算Attention Mask的过程如下： 首先，记录序列中每个图片起止token位置（包括初始0位置），得到两个位置序列为$ P_{s1}&#x3D;{0,2,5,9} $和 $ P_{s2}&#x3D;{0,5,11} $. 然后，分别用$ P_{s1} $和$ P_{s2} $来计算二维Attention mask矩阵，计算方式为：先初始化一个全0的mask矩阵，然后遍历每个$ P_{st} $,取[i,i+1]位置的两个数字（j,k）,使得矩阵行列坐标都满足在[j,k-1] 区间范围的位置置1。两个序列计算后的Mask矩阵，如下图所示。 计算好了上面的Attention Mask矩阵，在过Vision Encoder网络时，将Attention Mask作用在Attention计算上，就会隔离同一序列中不同图像的Attention计算。 2D-RoPE位置编码 (不是特别懂) RoPE不是特别懂！ 在Qwen2-VL系列的ViT网络中，并没有沿用Qwen-VL的2D绝对位置编码，而是引入了2D-RoPE相对位置编码。之所以引入2D-RoPE，我个人理解主要考虑Qwen2-VL系列处理的图片Patch是变长的，对于超长的一些位置，如果采用绝对位置编码，由于数据稀疏性， 并不能得到充分训练。但RoPE本身是具有一定的外推性，对长序列建模有更好的泛化能力。 1维的旋转位置编码（1D-RoPE）对序列增加相对位置的处理过程。这里简单引用苏神的推导结论： 首先对序列的每个位置构建分块矩阵，形如： 其中 m表示序列的位置， $ \\theta_i $ 沿用Sinusoidal位置编码的取值, d为位置编码向量的维度。 在计算Attention时，计算q,k乘积前，要首先对 q,k 做变换，也就是给 m位置的 q 乘矩阵 Rm，给 n位置的k乘以矩阵 Rn 。这样计算的 q,k通过增加绝对位置的变换，实质上是增加了相对位置信息。如下公式： 由于上述 Rm 变换矩阵比较稀疏，直接用矩阵乘法来实现会浪费算力，苏神也给出了一个推荐的实现方式，如下： 现在我们知道1维旋转位置编码RoPE的计算方式，那么怎么扩展到2维呢？参考苏神另一篇博客（详见：二维位置的旋转式位置编码）。RoPE从1维扩展到2维一个简单的结论：针对一个位置 （x,y） ，对维度为 d 的输入向量分成两半，前一半向量用 x 的一维RoPE矩阵( Rx )处理，后一半向量用 y 的一维RoPE矩阵( Ry )处理，然后再将两半处理后的结果拼接在一起，就做完了2维的RoPE处理。（相对与一维RoPE，扩展到二维，操作是比较简单，具体原理上请参考苏神的博客) 输入投影层：压缩Vision token + MLP Adapter Qwen-VL在输入投影层做了Vision token的压缩处理，是采用了Cross-Attention的架构，通过一个组可学习的Query向量来压缩原始的特征序列。那么Qwen2-VL为什么没有继续沿用Cross-Attention的架构？ 这里主要是因为Cross-Attention架构适合处理固定长度的 k,v ，当 k,v 长短不一时，是不适合做Attention计算的。而Qwen2-VL通过原生动态分辨率方法处理的每个图片的token序列恰恰是变长的，无法使用Cross-Attention架构做特征压缩处理。 Qwen2-VL采用了一种更简单的压缩方法：对空间位置临近的patch 特征做拼接，再经过2层MLP线性变换，这样将原来长度为 n 的序列，可压缩到 n&#x2F;4 ，最终将压缩后的特征序列输入给LLM模型。处理过程如图所示： 为了区分Vision token和文本token，Qwen2-VL也引入了两个特殊的token，&lt;|vision_start|&gt;和&lt;|vision_end|&gt;来标识Vision token。 对于一个 224224，如果ViT的 patch_size &#x3D; 14，最终将图片编码成一个66个token的序列输入到模型。具体计算过程：1.Patch 处理后的Token数为： (224&#x2F;14)(224&#x2F;14) &#x3D; 16*16 &#x3D; 2562.经过输入投影层压缩处理： 256&#x2F;4 &#x3D; 643. 最后再加上 个起止位置的特殊token：64+2 &#x3D; 66 Multimodel Rotary Position Embedding(M-RoPE) Qwen2-VL模型输入增加了视频模态，视频可以看做是在图片二维空间上，增加了时序维度，是三维时空分布的数据: (T,H,W)，M-RoPE将位置编码信息从1维扩展到了3维，这样就能清晰刻画视频模态数据时空位置信息。对于文本（一维）和图像（二维）的数据如何统一表示成3维的位置ID呢？处理也比较简单直接： 文本：因为文本是一维空间序列，三个维度的值保持一致，也就退化成1D-RoPE。 图像：图像只有宽高两个维度，所以对于一张图片，时序维度 的位置始终保持固定。 对于混合多模态数据，每个模态的起始position ID是前面模态三维位置ID中取最大的ID并加1得到。 有了三维的位置，最终怎么映射成3D-RoPE，映射方式类似与2D-RoPE，针对一个位置（x,y,z)，对维度为d的输入向量分成三份，前一份向量用x的一维RoPE矩阵( Rx )处理，中间一份向量用 y 的一维RoPE矩阵( Ry )处理，最后一份向量用 z 的一维RoPE矩阵（ Rz ）处理，然后再将三份处理后的结果拼接在一起，就做完了3维的RoPE处理。 统一的图像和视频理解框架； Qwen2-VL统一了视频和图像的理解框架，能混合输入图像和视频数据进行理解。为了保证图片和视频的处理一致，对视频和图像分别做如下处理： 视频处理：以每秒两帧的速率对视频进行采样，最终可采样偶数个帧序列。对于长视频为了平衡序列长度和计算效率，通过动态调整每一帧的分辨率，将视频总token限制在16K以内。 图像处理：对图像做复制操作，使得单一图片，变成一个时序为2的帧序列。 使用3D的卷积对帧序列做特征抽取，如图所示，每两张图片为一组进行卷积操作抽取特征。这样通过将卷积核扩充了时序维度，可以进一步压缩序列长度，因此也能进一步提升模型处理更多帧的能力。 模型训练 Qwen2-VL采用了与Qwen-VL一致的三阶段训练方式，但是Qwen2-VL在训练数据上相比Qwen-VL做了大量的有价值的工作。 数据来源除了获取开源数据、经过清洗的网页数据，还做的大量数据合成的工作。数据涉及多种场景，包括图像-文本对，OCR数据，视觉问答数据，视频对话数据等多样化数据。 此外Qwen2-VL数据规模大幅提升，Qwen-VL整体训练样本1.4B左右，Qwen2-VL直接翻了3个量级达到了1.4T。 通过大幅提升样本规模和样本多样性，使得Qwen2-VL的模型效果在多任务的评估中保持领先，也碾压了GPT-4o的效果。 Qwen2.5 -VL Qwen2.5-VL可循的材料只有一篇官方的[博客]()，官方的一张图基本描述了相对于Qwen2-VL的一些更新。Qwen2-VL 相比，Qwen2.5-VL 增强了模型对时间和空间尺度的感知能力，并进一步简化了网络结构以提高模型效率。 升级点： 时间和图像尺寸的感知 在空间维度上，Qwen2.5-VL 不仅能够动态地将不同尺寸的图像转换为不同长度的 token，**还直接使用图像的实际尺寸来表示检测框和点等坐标**，而不进行传统的坐标归一化。这使得模型能够直接学习图像的尺度。在时间维度上，引入了动态 FPS (每秒帧数)训练和绝对时间编码，将 mRoPE id 直接与时间流速对齐。这使得模型能够通过时间维度 id 的间隔来学习时间的节奏。 User Detect all motorcyclists in the image and return their locations in the form of coordinates. The format of output should be like {“bbox_2d”: [x1, y1, x2, y2], “label”: “motorcyclist”, “sub_label”: “wearing helmat” # or “not wearing helmat”}. 更简洁高效的视觉编码器 视觉编码器在多模态大模型中扮演着至关重要的角色。我们从头开始训练了一个原生动态分辨率的 ViT，包括 CLIP、视觉-语言模型对齐和端到端训练等阶段。为了解决多模态大模型在训练和测试阶段 ViT 负载不均衡的问题，我们引入了窗口注意力机制，有效减少了 ViT 端的计算负担。在我们的 ViT 设置中，只有四层是全注意力层，其余层使用窗口注意力。最大窗口大小为 8x8，小于 8x8 的区域不需要填充，而是保持原始尺度，确保模型保持原生分辨率。此外，为了简化整体网络结构，我们使 ViT 架构与 LLMs 更加一致，采用了 RMSNorm 和 SwiGLU 结构。 对视觉语言模型进行了全面的评估，比较了 SOTA 模型以及同尺寸规模模型中表现最好的模型。在旗舰模型 Qwen2.5-VL-72B-Instruct 的测试中，它在一系列涵盖多个领域和任务的基准测试中表现出色，包括大学水平的问题、数学、文档理解、视觉问答、视频理解和视觉 Agent。值得注意的是，Qwen2.5-VL 在理解文档和图表方面具有显著优势，并且能够作为视觉 Agent 进行操作，而无需特定任务的微调。 在较小的模型方面，Qwen2.5-VL-7B-Instruct 在多个任务中超越了 GPT-4o-mini，而 Qwen2.5-VL-3B 作为端侧 AI 的潜力股，甚至超越了我们之前版本 Qwen2-VL 的 7B 模型。 总结：不得不说，Qwen2.5-vl是真的强，在一些多模态任务上绰绰有余，而且检测任务上都表现很好。 "},{"title":"github+Hexo免费构建个人网站","date":"2025-05-30T09:14:29.000Z","url":"/2025/05/30/github-Hexo%E5%85%8D%E8%B4%B9%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/","tags":[["Hexo","/tags/Hexo/"],["github","/tags/github/"],["个人网站","/tags/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/"]],"categories":[[" ",""]],"content":"Hexo 是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 Hexo + GitHub 文章发布原理 主要分为以下几个步骤： 1，hexo环境搭建； 2，github部署； 3，网站配置； 4，发表第一篇blog； 1 hexo环境搭建1.1 环境准备Hexo 基于 Node.js，搭建过程中还需要使用 npm（Node.js 已带） 和 git，因此先搭建本地操作环境，安装 Node.js 和 Git。 Node.js： Git： 下载 Node.js 和 Git 程序并安装，一路点 “下一步” 按默认配置完成安装。 1.2 安装Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 如果是mac用户，前面加上 sudo： 1.3 初始化博客安装 Hexo 完成后，你可以新建一个文件夹来存放hexo的程序文件，如hexo-blog； 然后，执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 新建完成后，指定文件夹的目录如下： 1.4 本地预览完成后依次输入下面命令，启动本地服务器进行预览： 该命令也可以简写为： 完成生成过后即可执行下列命令，在本地开启服务。 该命令也可以简写为： 访问 ，出现 Hexo 默认页面，本地博客安装成功！ 2 Github部署部署 Hexo 到 GitHub Pages，本地博客测试成功后，就是上传到 GitHub 进行部署，使其能够在网络上访问。 2.1 配置git打开 Terminal，取消 ssl 认证： 设置 user.name 和 user.email 配置信息： 生成 ssh 密钥文件： 然后直接三个回车即可，默认不需要设置密码。你应该会看到如下输出： 在C:\\Users\\用户名&#x2F;.ssh&#x2F;id_rsa.pub处找到生成的id_rsa.pub 密钥，复制其中全部内容。 2.2 配置github使用邮箱注册 GitHub 帐号并登录； 点击右上角加号图标，选择 New repository 创建一个新仓库，仓库名为：用户名.github.io，该 用户名 使用自己的 GitHub 帐号名称代替。 打开 GitHub_Settings_keys 页面，点击 New SSH key 按钮，Title 处为本台计算机取一个名字，然后将刚刚复制的 id_rsa.pub 内容粘贴进去，最后点击 Add SSH key。 2.3 配置hexo打开 blog 根目录里的_config.yml文件，也称为 站点配置文件。 在站点配置文件的最后，修改为如下形式并保存： 打开 Terminal 安装 Git 部署插件： 2.4 部署博客到 Github Page输入以下命令完成生成与部署： 完成后打开浏览器，在地址栏输入博客所在仓库的路径，即 “http:&#x2F;&#x2F;用户名.github.io”，即可通过互联网访问生成的博客。 3 网站配置打开 blog 根目录里的_config.yml文件，也称为 站点配置文件。下面只介绍一些必要的选项： 根据自己的博客需要更改设置。 3.1 网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述 keywords 网站的关键词。支持多个关键词。 author 您的名字 language 网站使用的语言。 timezone 网站时区。Hexo 默认使用您电脑的时区。 3.2 写作默认不需要更改。 参数 描述 默认值 new_post_name 新文章的文件名称 :title.md default_layout 预设布局 post titlecase 把标题转换为 title case false external_link 在新标签中打开链接 true external_link.enable 在新标签中打开链接 true external_link.field 对整个网站（site）生效或仅对文章（post）生效 site external_link.exclude 需要排除的域名。主域名和子域名如 www 需分别配置 [] filename_case 把文件名称转换为 (1) 小写或 (2) 大写 0 render_drafts 显示草稿 false post_asset_folder 启动 Asset 文件夹 false relative_link 把链接改为与根目录的相对位址 false future 显示未来的文章 true highlight 代码块的设置, 请参考 Highlight.js 进行设置 prismjs 代码块的设置, 请参考 PrismJS 进行设置 3.3 主题配置Hexo 默认的主题不太好看，不过官方提供了数百种主题供用户选择，可以根据个人喜好更换。 这里以 Card() 主题为例。详细配置方法可以参考 Theme Cards 参考手册() 可以使用 Git 拉取「Cards」，以后还可以使用 git pull 直接更新「Cards」。在站点根目录打开终端，并执行： 在站点配置文件中，将 theme 的值改为 cards。如果获取「Cards」时你将文件夹重命名为其他名字，请将 cards 对应为你重命名文件夹的名字。 4 发布第一篇blog常用指令 4.1 创建博客： 创建成功后，博客位于 &#x2F;source&#x2F;_post 打开该博客之后可以看到： 本地启动进行预览： 浏览器访问 ： 4.2 管理图片一篇情况下，需要在source下面新建image文件夹，进行放置图片，但是如果未来图片的数据增加，不好引用，不方便管理。那我们可以采用下面的方式： 如下修改 Hexo 博客目录中的 _config.yml，打开这个配置是为了在生成文章的时候生成一个同名的资源目录用于存放图片文件。 执行如下命令创建一篇新文章，名为test1 执行完成后在source_posts目录下生成了一个md文件和一个同名的资源目录(用于存放图片) 我们在test文件夹下面放一张图片 image.png 在test1.md中添加内容如下，演示了图片的引用方式。本文主要markdown语法： 这种方式的缺点就是，无法在markdwon里面展示，但是可以在web界面上展示： 本地启动： 浏览器访问 ，页面如下，文章添加成功 参考：  "},{"title":"影像组学Radiomics","date":"2025-05-30T07:59:21.000Z","url":"/2025/05/30/%E5%BD%B1%E5%83%8F%E7%BB%84%E5%AD%A6Radiomics/","tags":[["医学图像","/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"],["影像组学","/tags/%E5%BD%B1%E5%83%8F%E7%BB%84%E5%AD%A6/"]],"categories":[[" ",""]],"content":"1 影像组学介绍其实就是想把图像处理领域的知识点，应用到医学图像里面。当初影像组学方法是，现在人工智能方法也是。 随着精准定量医学影像技术的快速发展、图像识别技术和数据算法的不断更新，医学图像大数据的挖掘和分析得以实现，极大程度扩展了医学图像的信息量。基于对图像信息进行纹理分析后能够得到高通量的特征的特点，受基因组学以及肿瘤异质性的启发，2012年荷兰学者Lambin在先前学者工作的基础上提出了**影像组学(Radiomics) **的概念。 Radiomics: Extracting more information from medical images using advanced feature analysis Lambin认为“高通量地从医学影像中提取大量特征，通过自动或半自动分析方法将影像学数据转化为具有高分辨率的可挖掘数据空间”医学影像可以全面、无创、定量地观察肿瘤的空间和时间异质性。Kumar等又对影像组学的定义进行了拓展，影像组学是指从CT、PET或MRI等医学影像图像中高通量地提取并分析大量高级且定量的影像学特征。这个理念的提出在随后的七年迅速被越来越多的学者改进与完善。 影像组学，一共4个常见字。但4个字加在一起是啥意思？好吧，拆成“影像”和“组学”两个词来说。“影像”通常指的就是放射影像，主要包括了CT、MR影像，当然，现在也陆续有加入了PET、US影像研究。组学（Omics），专门百度了一下，通俗理解就是把与研究目标相关的所有因素综合在一起作为一个“系统”来研究。目前主要包括基因组学（Genomics），蛋白组学Proteinomics，代谢组学（Metabolomics），转录组学(transcriptomics），免疫组学（Immunomics），RNA组学(RNomics)，影像组学（Radiomics）等。 影像组学流程： 影像组学本质上来说其实是一种分析思路方法，从临床问题出发，最后回到解决的临床问题。一般分为五个主要处理步骤： 标准医学影像数据获取和筛选：数据收集前，首先需要根据明确的研究方向进行数据筛选，例如做肿瘤分型或肺炎分型的鉴别诊断，所选影像数据是否有病理或病原学检测金标准进行对照；做影像学疗效评估时，是否具有多期治疗相应的影像资料匹配等。 图像分割：指将图像分成若干个特定、具备独特属性的区域并提取感兴趣目标的技术和过程。根据研究目的的不同，图像分割的目标可以是病灶、正常参考组织或是组织解剖结构，可以是三维也可以是二维区域，影像组学随后的分析研究都围绕这些从图像内分割出来的区域进行。 特征提取：影像组学的核心步骤就是提取高通量的特征来定量分析ROI的实质属性。基于Image Biomarker Standardization Initiative (IBSI)标准[4]统计划分，常将影像组学特征分为形状特征(Shape features)、一阶统计学特征（First order statistics features）、纹理特征 (texture-basedfeatures)、高阶特征(high-order features)以及基于模型转换的特征。 机器学习：特征选择，上述通过特征提取，提取到的特征数量可能有几百到几万不等，而并不是每一个特征都与要解决的临床问题相关联；另一方面，在实践中，由于特征数量相对较多，而样本数量较少，容易导致随后的模型出现过拟合的现象，从而影响模型的准确率。特征选择是根据某些评估准则，从特征集中直接选取合适的子集，或者将原有的特征经过线性&#x2F;非线性组合，生成新的特征集，再从新特征集中选取合适的子集过程。 模型应用：针对医生具体的临床问题，在临床研究问题标签的基础上建立由上述特征筛选出来的关键特征，或进一步结合影像组学以外的特征（如临床体征、病理、基因检测数据）组合而成的预测模型。 2 影像组学方法流程及工具代码：具体影像组学的流程如下： 1，影像数据准备 2，感兴趣区域分割 3，特征提取与特征选择 4，建模分析 5，结果展示 2.1 影像数据获取不同设备厂商、同一厂商不同型号的影像设备在图像扫描和重建协议上往往存在着很大的差异，迄今为止仍然缺乏统一的采集标准规范。这就可能给包含影像组学在内的众多医工结合项目造成了潜在的影响。尽管对数据存储传输、图像勾画和算法运行速度带来诸多不便，但为了最后的建模评估步骤能达到更好的效果，仍然推荐入组分辨率更高、信息量更为丰富的薄层数据。这些数据最好具有相同（或相近）的采集参数，以尽可能降低数据质量被各种成像因素的干扰。目前，国内很多医院都开展过肿瘤的影像组学相关项目，比如肺癌、肝癌、结直肠癌等等。但随着组学科研在全国乃至世界范围内发生“内卷化”，课题总数据量和多中心研究都已经成为审稿中的决定性因素。这时，在繁杂的医院数据中，既要严控数据入组条件，同时也要满足项目所需的样本数量，只有兼顾两者才可能取得最优的效果。 此外，多模态影像组学时代的开启给医工两方面的研究者都提出了更大的挑战。数据量的成倍增加、算法工程师的影像专业背景，都可能成为项目推进的阻力。这里有非常重要的一点，要求影像医生做好数据筛选：所有的影像医工交叉项目，数据量的一大原则都是“**贵精不贵多**”——除了保留与病灶相关的必要序列外，不要有任何冗余的图像，否则会成为后续工作中非常不利的因素。至于质量不过关（如包含伪影、扫描序列不完整）、缺乏完整临床指标的样本，都是应该被排除在项目之外。 当然，你如果没有私有数据集的话，也有一些比赛公开了数据集。或者你可以用公开数据集做泛化性分析： 如： 这个项目地址公开了很多医学影像数据集。 2.2 感兴趣区域分割影像准备完毕，接下来的重要步骤是病灶ROI的勾画，从而对照影像序列，进一步开展后续的特征提取。医学图像分割经过几十年的发展，目前仍然保持了两大类别：手动&#x2F;半自动分割，以及全自动分割。 由于病灶的特质性和不规则性，手动&#x2F;半自动分割仍然是现有条件下的最佳选择，即使手动分割耗时较大，它仍然是金标准获取的唯一途径。课题中既可以采用3D Slicer、ITK-SNAP等传统分割软件，也可以尝试像Pair等新兴工具，甚至可以使用工作站日常诊断中的三维重建结果。全自动分割算法包括阈值&#x2F;区域生长、分水岭、水平集等，也有近年来非常火热的深度学习算法。 但很遗憾，尽管学术界和工业界都付出了相当大的努力，但迄今为止，仍然没有高精度、全自动的通用分割模型可以完美地应用于影像科研的日常流程和基础IT架构中，**自动分割后ROI还是要医生手动调整确认以保证项目的精度效果。** 如果当你面对大量数据集的标注时，你可以使用半自动+人工检验的方式进行金标准的制作。 或者你使用公开数据集的话，公开数据集本身就有分割标注，也不用你进行分割了。 2.3 特征提取与降维选择2.3.1 特征提取：影像组学特征由一系列传统图像特征的合集构成，既包含了一阶梯度特征（区域内统计信息）、形状特征（如形态学参数、圆形度等）、纹理特征（灰度共生矩阵、灰度区域大小矩阵、灰度游程矩阵），也包含了各种图形滤波变换后的特征，比如高斯-拉普拉斯变换、小波变换、平方根滤波等。这些特征都曾经在医学图像分析中有过较为成功的运用，而影像组学正是把高阶的图像特征聚集在一起、进一步提高分析结果质量的方法。每个影像组学特征都有着独立的计算公式，现在在网上已经可以找到丰富的特征提取工具直接调用，而无需自行学习理解公式、手动编程计算。 时下流行的时是**PyRadiomics组学工具包。使用起来方便快捷，而且是基于python。** PyRadiomics的官方文档: 具体使用代码可以参考： 【使用pyradiomics提取影像组学特征【详细】】  2.3.2 特征降维(特征选择)面对大量的特征，我们不可能对其直接进行模型构建，我们需要对数据进行降维，给数据“瘦身”，从成千上万的特征数据中获取最为有价值的特征数据。 提取完毕的影像组学特征，少则成百上千；有些“噱头”概念也会在多模态+各种图像预处理后形成高达十万数量级的特征维度；目前还有一种流行的方法是将组学特征和临床数据相互结合，以便最大化综合各种层面的信息优势。这个时候就需要采用一些特征降维和筛选的手段来对特征数目进行一定的限制，避免冗余数据影响机器学习模型的精确和稳定性。 常见的降维和筛选方法有主成分分析、相关性分析以及基于L1惩罚项的特征选择法等——其实当维度达到一定级别的时候，再增加特征对最终的结果影响已经降到很低，反而严重增加了完成组学流程的时间和空间复杂度。 2.4 建模分析：课题中需要将样本分为训练集和测试集（遵循着7:3或8:2的原则），如果是多中心课题，可以根据不同医院区分样本，以其它医院的数据做外部验证的测试集，评价建模的鲁棒性。 建模时首先使用训练集，通过学习一个函数，找到各类样本的最佳区分“界面”。常用的分类器包括了决策树、逻辑回归、随机森林、支持向量机（SVM）等。在建模过程中，为了减少过拟合和选择偏差、保证更好的模型性能，常常会引入交叉验证（Cross Validation）的方法。交叉验证将数据的训练集样本切割成若干较小子集，然后先在一个子集上做分析，而其它子集则用来做后续对此分析的验证，并确定一些模型的参数。 2.5 结果分析模型训练完毕后，再采用测试集进行组学预测结果和临床标签的对比，进行性能评估。常用于展现影像组学结果的图或数值有ROC曲线、（95%置信区间下的）AUC值、敏感度、特异度、特征贡献度、相关系数热图等。 参考：【影像组学初学者指南】  【组学浅析之通俗理解影像组学的定义】  【影像组学十周年：技术，应用与展望】 "},{"title":"SimpleITK的使用","date":"2025-05-30T07:15:53.000Z","url":"/2025/05/30/SimpleITK%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":[["医学图像","/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"],["医学图像预处理","/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/"]],"categories":[[" ",""]],"content":"1，前情提要SimpleITK是专门处理医学影像的软件，是ITK的简化接口，使用起来非常便捷，SimpleITK支持8种编程语言，包括c++、Python、R、Java、c#、Lua、Ruby和TCL。本文主要以python为例进行讲述。 在SimpleITK中，图像的概念与我们在计算机视觉中常用的RGB图像差异很大，后者只是一个多维矩阵，是一个数学上的概念，而在SimpleITK中图像是一个物理实体，图像中的每一个像素都是物理空间中的一个点，不光有着像素值，还有坐标、间距、方向等概念。 SimpleITK的项目地址github： 说明文档：SimpleITK Sphinx Documentation — SimpleITK 2.0rc2 documentation 2，SimpleITK基础概念参考SimpleITK文档：Fundamental Concepts — SimpleITK 2.0rc2 documentation 2.1 医学图像基础概念SimpleITK假设世界坐标系下的长度单位为mm。 世界坐标系：用于医学成像仪器的坐标系。坐标用元素是连续值的向量表示。 图像坐标系：用于表示像素点的坐标系。坐标用元素是离散值的向量表示。 Origin - 世界坐标系下，图像相对于坐标系原点的位置。 Spacing - 两个像素之间在世界坐标系下的间隔。因为图像数据以离散值存储，而像素间隔取决于成像仪器而各不相同。需要注意的是各个方向的Spacing通常是不相等的，例如(0.67777,0.67777,1.33333)，其实也就是体素的大小。 Size - 每个维度的大小。用元组表示。 Direction cosine matrix - 方向余弦矩阵。世界坐标系坐标轴对应于图像像素矩阵的方向。用一组矩阵空间内的基表示。通常每个向量形似(1,0,0).T。 这些元数据元素中的每个元素的含义在下图中可视化地说明： 2.2 通道&#x2F;Channels同自然图像格式中的通道概念。医学通道中也有通道的概念。比如使用的CT图像是单通道图像，可视化后可用灰度值表示。但是也有一些成像仪器生成的图像具有多个通道，例如RGB色彩格式，能够展现更丰富的特征，相应的占用空间也会3。（比如说，LUNA16中单个CT图像文件的大小约200MB。其纵向切片分辨率为512512，切片数为120 ~ 700，通道数为1。可以看到低分辨率下单张图像的数据量就已经很夸张了。） 2.3 空间域变换&#x2F;TransformsSimpleITK 支持两种类型的空间转换，一种具有全局（无界）空间域，可以使用各种全局2D 和3D 转换（平移、旋转、刚性、相似性、仿射… …）。另一种具有有界空间域。SimpleITK 中的点通过转换使用 TransformPoint 方法进行映射。其中包括 B 样条变形变换，通常称为自由变形，以及位移场变换。 2.4 重采样&#x2F;Resampling重采样是对图像进行采样的动作，而图像本身就是对原始连续信号的采样。可以通过重采样将改变体素的大小。虽然 SimpleITK 提供了大量的插值方法，但是最常用的两种方法是 sitkLinear 和 sitkNearestNeighbor。前者用于大多数插补任务，是精度和计算效率之间的折衷。后者用于插值表示分割的标记图像。这是唯一的插值方法，不会引入新的标签到结果。 2.5 图像配准&#x2F;Registration OverviewSimpleITK包含了一些图像配准的功能。 医学图像配准（Medical Image Registration）是指将不同的医学图像（根据定义，计算机内从像素到meta信息有任意不同之处的两张医学图像都是不同的。在现实中的定义则是两次不同成像过程产生的图像。）进行对齐和匹配。医学图像配准的目标是将两个或多个医学图像进行对齐，使它们共享相同的解剖结构、几何形状和空间定位信息，从而实现它们之间的定量比较、分析和整合。常用的医学图像配准方法包括基于特征的方法、基于强度的方法、基于形变场的方法等。简单地说，来自于同一个病人的两张图像可能各自含有一些对方没有的重要信息，图像配准技术的目标是能够同时利用这两张图象的信息。如果两张图象来自于不同原理的成像仪器（例如CT和MRI），这种图像配准技术被称作跨模态图像配准（Cross-Modality Image Registration）&#x2F;多模态图像配准（Multimodal Image Registration）。跨模态指的是A-&gt;B或B-&gt;A，多模态则是指任何涉及到多个模态的问题环境。可能是因为原始图像采样过程中丢失了精度，或者数学上两张不同的医学图像本身就不可能完全重叠？在图像配准任务中，实现的过程用估计（estimate）来表述。图像配准最基本的过程包含了坐标轴位移、旋转、拉伸等操作。用SimpleITK完成图像配准工作需要Transformation、Similarity metric、Optimizer、Interpolator这四个工具共同实现。配准更具体细节参考：Registration Overview — SimpleITK 2.4.0 documentation 3，SimpleITK的基本使用3.1 安装与常见属性在这里，使用python对SimpleITK库中常用的函数进行举例说明： 在使用SimpleITK库之前，需要将SimpleITK库导入进来，如下： 3.1.1 sitk中的常见属性值sitk中有以下四种常见的属性，分别可以使用get的方式获取，代码如下所示： 举例说明：以二维图像为例，左下图是世界坐标系，右下图是图像坐标系。属性值如下所示 3.2 读取和保存图像SimpleITK可以读取如.mhd , .nii, .nrrd等图像数据格式。 Note1：图像访问是以x， y， z顺序GetPixel(x,y,z) 或 **image[x,y,z]**， 从0开始索引。 Note2：默认的mask图像类型和默认值为sitkUInt8或标量图像uint8_t， 默认值为0和1，其中1代表的是mask。 使用SimpleITK读取和保存Nii.gz文件: 3.3 像素类型像素类型表示为枚举类型，下面是部分枚举类型的表。 还有sitkUnknown类型，用于未定义或错误的像素ID，值为-1。 Note：64位整数类型并非在所有的发行版上都可用，如果不可用，则值为sitkUnknown，将图像保存为nii文件，用ITKsnap读取时就会出现的错误如下： 3.4 SimpleITK图像数据和Numpy矩阵数据之间的转换一般我们会用SimpleITK读取图像，再转换为numpy矩阵格式，这样方便数据的处理。 Note1：在SimpleITK中，各术语对应如下： Width: 宽度，X轴，矢状面（Sagittal） Height: 高度，Y轴，冠状面（Coronal） Depth: 深度， Z轴，横断面（Axial） Note2: SimpleITK图像顺序是x，y，z三个方向的大小（在第一节中也讲过），而numpy矩阵的顺序是z，y，x三个方向的大小, 要注意索引位置。 举个例子：假设实验用的图片大小为32025080，即矢状面（x轴方向）切片数为320，冠状面（y轴方向）切片数为250，横断面（z轴方向）片数为80。 3.4.1 SimpleITK2Numpy:GetArrayFromImage()：返回图像数据的副本。然后可以自由地修改数据，因为它对原始SimpleITK图像没有影响。 3.4.2 Numpy2SimpleITK：GetImageFromArray()：返回一个SimpleITK图像，原点设置为0，所有维度的间距设置为1，方向余弦矩阵设置为identity，强度数据从numpy数组中复制。在大多数情况下需要设置适当的元数据值。 Note: 从array转成image需要设置原点，维度间距以及方向。 3.5 访问像素和切片两种方式：一是使用GetPixel和SetPixel函数，二是使用python的切片操作符。例子如下： 3.6 图像重采样重采样目的是将医疗图像中不同的体素归一化到相同的大小，可分为两种方案， 方案一：按目标Spacing缩放 方案二：按目标Size缩放。 这两种方案具体操作分为三个步骤： 使用SimpleITK读取数据，获得原始图像的Spacing以及Size； 如果是方案一，则图像原始Size乘以原始Spacing除以新Spacing得到新Size，如果是方案二，则图像原始Size乘以原始Spacing除以新Size得到新Spacing； 最后将新Spacing和新Size赋值到读取的数据即可。 下面以指定Spacing大小，对原始数据进行重采样，例子如下： 3.7 图像分割图像二值化分割： 是分割方法中最基础的，通过定义lowerThreshold和upperThreshold两个像素临界点，只要像素值在两者之间，则该像素值改为insideValue，否则改为outsideValue。这种方法只是简单的基于灰度范围标记图像像素，不考虑几何或连通性。 图像区域生长分割： 需要确定种子点、生长准则和终止条件。具体来说，对每一个需要分割的区域找一个种子像素作为生长的起点，根据生长准则将种子像素邻域中与种子像素具有相同或相似的像素合并到种子像素所在的区域，直到没有满足条件的像素可以被包进来就终止。 在SimpleITK中，首先会计算当前区域中包含的所有像素点灰度值的标准差和期望，通过定义multiplier因子（乘以标准差）来计算以期望为中心的灰度值范围，如果initialNeighborhoodRadius邻域半径内的灰度值位于这个范围就被包进来，灰度值改为replaceValue，当遍历了所有邻域像素，即认为完成了一次迭代，下一次迭代时，像素点的灰度值期望和标准差是以新的像素区域为基础进行计算的，一共要迭代numberOfIterations次。 3.8 图像的形态学操作图像形态学操作一般常用的是开，闭，膨胀，腐蚀操作： 3.9 连通域分析连通域分析一般是针对二值图像，将具有相同像素值且相邻的像素找出来并标记，其中二维图像连通域一般包括4连通和8连通，三维图像连通域包括6连通、18连通和26连通。 4 参考SimpleITK Sphinx Documentation — SimpleITK 2.0rc2 documentation  SimpleITK图像基础（三）——SimpleITK学习笔记_Jiaxxxxxx的博客-CSDN博客 SimpleITK的使用"},{"title":"test1","date":"2025-05-30T07:05:52.000Z","url":"/2025/05/30/test1/","categories":[[" ",""]],"content":"这是图片测试： "},{"title":"Hello World","date":"2025-05-18T12:49:22.161Z","url":"/2025/05/18/hello-world/","categories":[[" ",""]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"}]